\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\select@language {english}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces A third order tensor.\relax }}{1}{figure.caption.13}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Fibers and slices of a tensor according to each mode.\relax }}{2}{figure.caption.17}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Tensor unfolding or matricization along different modes.\relax }}{7}{figure.caption.61}
\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of a \emph {k}-mode product i.e., a tensor-matrix multiplication of a 3-dimensional tensor.\relax }}{8}{figure.caption.65}
\contentsline {figure}{\numberline {1.5}{\ignorespaces An example of a tensor-vector product.\relax }}{8}{figure.caption.68}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Representation of third-order tensor with an outer product of vectors.\relax }}{9}{figure.caption.71}
\contentsline {figure}{\numberline {1.7}{\ignorespaces A Tucker decomposition of a three-modes tensor\relax }}{11}{figure.caption.76}
\contentsline {figure}{\numberline {1.8}{\ignorespaces An Higher Order SVD of a third-order rank-(R1-R2-R3) tensor and the different spaces, from Tensorlab \parencite {WTensorlab}.\relax }}{13}{figure.caption.84}
\contentsline {figure}{\numberline {1.9}{\ignorespaces Tensor Decompositions for speeding up a generalized convolution. Each box correspond to a feature map stack within a CNN, (frontal sides are spatial dimensions). Arrows show linear mappings and demonstrate how scalar values on the right are computed. Initial full convolution (A) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d \IeC {\texttimes } d window over all input maps. Jaderberg et al. (B) approximate the initial convolution as a composition of two linear mappings in which the intermediate mpa stack has R maps, being R the rank of the decomposition. Each of the two-components computes each target value with a convolution based on a spatial window of size dx1 or 1xd in all input maps. Finally, CP-decomposition (C) by Lebedev et al. approximates the convolution as a composition of four smaller convolutions: the first and the last components compute a standard 1x1 convolution that spans all input maps while the middle ones compute a 1D grouped convolution \textbf {only on one} input map. Each box is mathematically described in equation (\ref {eq:cpd1}-\ref {eq:cpd2})\relax }}{15}{figure.caption.99}
\contentsline {figure}{\numberline {1.10}{\ignorespaces Tucker-2 Decompositions for speeding-up a generalized convolution. Each box corresponds to a 3-way tensor $X, Z, Z^' and Y$ in equation (\ref {eq:tucker1}-\ref {eq:tucker3}). Arrows represent linear mappings and illustrate how each scalar value on the right is computed. Red tube, green cube and blue tube correspond to 1x1, dxd and 1x1 convolution respectively.\relax }}{19}{figure.caption.125}
\contentsline {figure}{\numberline {1.11}{\ignorespaces Evaluation of the {'rankest'} method for different sizes of input and output maps. Note that the maps sizes can be even higher in CNNs. Clearly, an iterative rank estimation approach does not scale well.\relax }}{22}{figure.caption.148}
\contentsline {figure}{\numberline {1.12}{\ignorespaces Mode-1 (top left), mode-2 (top-right), and mode-3 (bottom left) tensor unfolding of a third-order tensor. \textbf {Not shown}: after unfolding, the rank $R$ of the resulting matrices is computed with VBMF. (Bottom right): Then, given the rank $R$, the Tucker decomposition produces a core tensor $\mathcal {S}$ and factor matrices $A^(1)$, $A^(2)$ and $A^(3)$ of size $I_1 \times R$ $I_2 \times R$ and $I_3 \times R$ respectively. \relax }}{24}{figure.caption.152}
\contentsline {figure}{\numberline {1.13}{\ignorespaces Comparison of different optimization algorithms to compute tensor decomposition on real datasets. The method proposed by Liu et al. outperforms other methods by a large margin.\relax }}{25}{figure.caption.154}
\contentsline {figure}{\numberline {1.14}{\ignorespaces Comparison of different optimization algorithms to compute CPD; between yellow and blue line only the init method is diverse. It is evident how ALS is indeed faster but more unstable and less precise.\relax }}{26}{figure.caption.157}
\contentsline {figure}{\numberline {1.15}{\ignorespaces Comparison of CP-NLS decomposition for different ranks: the higher the \emph {R}-terms, the higher is the accuracy. For $R$=500 the decomposition is much more accurate than other ranks.\relax }}{27}{figure.caption.158}
\contentsline {figure}{\numberline {1.16}{\ignorespaces Accuracy and timing comparison between various CP-decomposition algorithms. ALS is always fast but much less accurate than CP-OPT. Image from \parencite {WCPD-talk}.\relax }}{27}{figure.caption.159}
\contentsline {figure}{\numberline {1.17}{\ignorespaces A generic Tensor Decomposition (TP) block micro-architecture. The first and last layers performs channels squeezing/restoration while the actual convolution is performed inside these two layers. This convolution can be either separable or standard; either way the cross-channel parameters redundancy is exploited. \relax }}{29}{figure.caption.162}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Architettura del framework Torch\relax }}{37}{figure.caption.352}
