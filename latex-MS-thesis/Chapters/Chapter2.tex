

% Chapter Template

\chapter{State of the art} % Main chapter title
\label{Chapter2}
\def \teoria {Figures/teoria}
\def \path	 {Figures/C2}

In recent years tremendous progresses have been made with regard to CNN model compression and acceleration. In this chapter an overview of the most significant techniques is provided. Note that details and terminology of CNNs are later explained in chapter 3. 

%--------------------------------------------------------------------%	SECTION 1
%--------------------------------------------------------------------
\section{Model Design vs. Model Compression}
Convolutional neural networks compression can be addressed in two main different ways: the first one is to focus on exploring the best practices to build smaller networks without sacrificing accuracy; the other one consists of taking a pre-trained successful model and compress the existent weights to extract a thinner version of it but with same knowledge, and so the accuracy. \\
This is the first point to keep in mind. 
\newline 
The other distinction is based on which layers to address, namely convolution layers and fully-connected one (layers detailed explanation will follow in chapter 3). This depends on which, among the following, is the priority: 
    \begin{itemize}
        \item  \textbf{acceleration}: most of the computation in CNNs are done in the first convolutional layers; thus these need to be compressed in order to achieve a significant speedup; 
        
        \item \textbf{compression}: most of the parameters are in the last fully connected layers; therefore a reduction on the former is required to save on memory consumption.
    \end{itemize}
    
For the reasons above, each of the presented schemes will be labeled with the possible area of application. 

\newline 
The approaches proposed so far can be classified into four main categories: \emph{(i)} parameter pruning; \emph{(ii)} low-rank factorization; \emph{(iii)} knowledge distillation; \emph{(iv)} compact network design.

%--------------------------------------------------------------------
%	SECTION 2
%--------------------------------------------------------------------

\section{Parameter pruning and sharing}
Pruning was proposed before deep learning became popular, and it has been widely studied in recent years \parencite{brain}. Based on the assumption that lots of parameters in deep networks are unimportant or unnecessary, pruning methods try to remove parameters that are not crucial to the model performance. 
\newline 

In this way, networks becomes more sparse and thus show few benefits: the sparsity of the structure acts as a regularization technique against over-fitting, hence improving the model generalization; the computations involved in the pruned parameters are omitted, thus the computational complexity can be reduced; finally, sparse parameters require less disk storage since they can be stored in compressed formats. 
\newline 

Pruning methods can be applied on both model design and compression and since they address single weights, can be applied on both fully connected and convolution layers. Moreover, they can be further classified into three categories: network pruning, model quantization and structural matrix design.  

\subsection{Network Pruning}
\label{subsec:pruning}
As aforementioned, the core assumption of pruning is that many parameters are redundant and unnecessary. However, techniques differ on how to assess which parameters are less important than others and the granularity of the actual pruning. 

Figure \ref{fig:pruning} shows the various pruning levels for a convolutional layer.

\begin{figure}[h!]
 \centering
 \includegraphics[width=1.0\textwidth]{\path/pruning.jpg} 
 \caption{Different pruning granularity for a $3 \times 3 \times 3$ convolutional filter. Image from \parencite{survey2018}}
 \label{fig:pruning}
\end{figure}

\newline 

\paragraph{Fine-grained pruning}
Fine-grained pruning methods remove parameters in an unstructured way, i.e., any unimportant parameters in the convolutional kernels can be pruned without particular constraint. This gives large degrees of freedom but requires more manual parameter tuning. 

An early approach to network pruning was the biased weight decay \parencite{pruning-biased} that consisted in cutting away weights in a magnitude-based fashion. Later works, as the Optimal Brain Damage \parencite{brain-damage} and the Optimal Brain Surgeon\parencite{brain-surgeon}, assessed the search of unimportant parameters based on the Hessian of the loss function and show better results. 
\\
However, it is unaffordable for deep networks to compute the second order derivatives for each weight due to a huge computational complexity.  

A recent improvement that enabled to prune uninformative weights in a pre-trained CNN model, was proposed by Han et al. in \parencite{deep-compression}. Their \emph{"deep compression framework} consists of three important steps: 
    \begin{enumerate}
        \item pruning redundant connections and retrained the sparse connections; 
        
        \item quantization of the remaining weights;
        
        \item Huffman coding to encode the quantized weights in a loss-less format.  
    \end{enumerate}

By using the former method, Han et. al managed to compress AlexNet by 35$\times$ with no drop in accuracy; this method achieved state-of-the-art  The pipeline of the former is shown in figure \ref{fig:huffman}


\begin{figure}[h!]
 \centering
 \includegraphics[width=1.0\textwidth]{\path/huffman.jpg} 
 \caption{The three steps compression method proposed in \parencite{deep-compression}: pruning, quantization and encoding.}
 \label{fig:huffman}
\end{figure}

\paragraph{Group-level pruning}
Group-level pruning apply the same sparse pattern on the filters, so that convolutional filters can be represented as thinned dense matrix. An example of this can be seen in figure \ref{fig:group-pruning}.

\begin{figure}[h!]
 \centering
 \includegraphics[width=0.7\textwidth]{\path/group-level.png} 
 \caption{An example of group-level pruning, from  \parencite{survey2018}.}
 \label{fig:group-pruning}
\end{figure}

Since this type of pruning produces thin dense matrices, convolution operation can be implemented as thinned dense matrix multiplication leveraging on the Basic Linear Algebra Subprograms (BLAS) and hence achieving higher speed-ups (almost linear with the sparsity level). 

One way to implement this, is by employing group-sparisity regularizers. As explained in chapter 3, regularizers are added to the loss function in order to penalize too complex models. By applying a group-sparsity regularizer, Lebedev and Lempitsky \parencite{lebedev2} showed how to train a CNN easily with group-sparsified weights. 


\paragraph{Filter-level pruning}
Filter-level pruning, as the name suggests, prunes the individual filters (or channels) of the CNN, thus making the network thinner. After pruning filters on one layer, the following layers' channels are also pruned. For this reason, compared to other pruning, this method is more suited to accelerate deep networks 
\newline 

In order to properly choose the filter channels to prune, custom parameters have to be imposed. For example, it's possible to guide a layer filter pruning by using the next layer's feature map as a guide i.e., by minimizing the reconstruction error on the latter. In this way, the layer will optimize a subset of its filters to obtain the same result as the original model. This strategy has been proposed by \parencite{Luo2017}. 
\newline

Other methods have always applied similar per-layer constraints. 


\paragraph{Considerations}
\begin{itemize}
    \item \textbf{Application}: convolutional and fully-connected 
    
    \item \textbf{Drawbacks}: Pruning techniques are elastic and be applied on every layer and model. However, all pruning criteria require a careful manual setup of sensitivity per-layer, which demands fine-tuning of the new imposed parameters. This can be inconvenient in some scenario. 
\end{itemize}


\subsection{Quantization and Binarization}
Network quantization involves compressing the original network by reducing the number of bits required to represent each weights. This strategy is further classifiable into two groups: \emph{scalar and vector} quantization and \emph{fixed-point} quantization. 

\paragraph{Scalar and vector quantization}
Scalar and vector quantization technique has a long history, and it was original used for data compression. Through this method the original data can be represented by two basic elements: 
 \begin{itemize}
     \item \emph{a codebook} that contains a set of "quantization centers";
     
     \item \emph{a set of quantization codes} used to indicate the assignment of the quantization centers.
 \end{itemize}
 
 Most of the time, the cardinality of quantization centers is far smaller than the number of original parameters. 
 \\
 In addition, the quantization codes can be encoded by lossless encoding methods as Huffman coding, which was also a fundamental gear of the deep compression pipeline mentioned in section \ref{subsec:pruning}. For this reason scalar and vector quantization can achieve high compression ratio and can be applied on both convolutional \parencite{WU2016} and fully-connected layer \parencite{gong}. 

\paragraph{Fixed-point quantization}
Resource consumption of CNN is not only based on the rough number of weights, but also on the activations and the gradient computation during the backward pass in the training phase. To tackle this, fixed-point quantization methods are divided into:
\begin{enumerate}
    \item quantization of weights; 
    
    \item quantization of activations; 
    
    \item quantization of the gradient: these methods are also able to speed-up training, for obvious reasons. 
\end{enumerate}

Compression with 8-16-32 bits fixed-point have been experimented with fair results. [CIT NEEDED]. Interestingly, it has been proposed that 16-bit fixed-point was adequate to train small models, while 32-bit fixed-point were required in the case of bigger nets.  


In the extreme scenario of 1-bit representation of each weight, that is, \emph{binary weight neural networks}, there are also many works that directly train CNNs with binary weights, for instance, BinaryConnect \parencite{binaryconnect}, BinaryNet \parencite{binarynet} and XNOR-Networks \parencite{XNOR}. 

The main idea is to directly learn binary weights or activations during the model training, so that it is possible to get rid of floating computations once for all. 
\newline 

Among all, XNOR-net achieved remarkable results on the ImageNet dataset, outperforming other quantized nets by a large margin. It reached the same accuracy of AlexNet, while saving 32$\times$ memory usage and 58$\times$ faster convolution operations. An overview of XNOR-net is reported in figure \ref{fig:xnor}

\begin{figure}[h!]
 \centering
 \includegraphics[width=1.0\textwidth]{\path/xnor.jpg} 
 \caption{XNOR-Net overview with computation speedups and memory savings. }
 \label{fig:xnor}
\end{figure}

An interesting point made by XNOR-net is that it is the first state-of-the-art network that can be trained on CPUs and can be a candidate for real-time operations. It is indeed already been deployed on smartphone by \parencite{Wxnor-ai}.


\paragraph{Considerations}
\begin{itemize}
    \item \textbf{Application}: convolutional and fully-connected 
    
    \item \textbf{Drawbacks} the accuracy of said binary nets is significantly lowered when dealing with large CNNs such as GoogLeNet. Besides, the approximation techniques for binary weights don't yet take into account the effects on accuracy loss
\end{itemize}
. 

%--------------------------------------------------------------------
%	SECTION 3
%--------------------------------------------------------------------

\newpage
\section{Knowledge Distillation}
Knowledge distillation is different from other methods since it tries to built a smaller model that can have a totally different architecture but same accuracy on the same problem domain. 
\\
Its mechanism resembles much more what happens in the human world: in fact, it works by transferring the knowledge from a large network, \emph{the teacher}, to a much smaller one i.e., \emph{the student}. Hence, it is also called teacher-student network. 

Originally introduced by Caruana et al. \parencite{caruana} only on shallow models, it has been now reproposed. By utilizing what is known as the \emph{"dark knowledge"} transferred from the teacher network, the smaller model can achieve higher accuracy than training merely by the "raw" class labels.

As of 2018, three main works have paved the road to this interesting application: 
\begin{enumerate}
    \item Hinton et. al \parencite{hinton-KD} proposed to improve the student network training with the \emph{softmax} layer's output of the teacher, i.e. the (log)probabilities of each class. This scheme is reported in figure \ref{fig:KD}.
    
    \item Following this line, Romero et al. \parencite{romero-KD} proposed \textit{FitNets} as teachers for thinner and deeper networks. They went further by training the student not only on the softmax probabilities of the teacher but also on its intermediate feature maps.
    
    \item Finally, in \parencite{greci-KD} Zagorukyo and Komodakis proposed an even more interesting approach: the student network was trained on the \emph{attention maps}. Attention  maps are defined as those feature maps where the network held most of its knowledge. Once these maps are found, we can transfer only their content to the student, resulting in an even thinner model. This new branch goes with the name of \emph{Attention Transfer} (AT).
\end{enumerate}

Moreover, the teacher can be an ensemble of models \parencite{Wensemble} with modules called "specialists" who focus on only training the student on sub-parts of the problem. 
Noticeably, these methods did not show the need for regularization techniques such as dropout (see Chapter \ref{Chapter3}) since the transferred knowledge, in a sense, is already filtered and optimized. 


\begin{figure}[h!]
 \centering
 \includegraphics[width=0.7\textwidth]{\path/KD.png} 
 \caption{Teacher-student scheme: the student learn on a mix of }
 \label{fig:KD}
\end{figure}


\paragraph{Considerations}
\begin{itemize}
    \item \textbf{Application}: training from scratch only.
    
    \item \textbf{Drawbacks}: 
Training with KD technique is way less costly. However, currently these approaches can only be applied to classification tasks with softmax loss functions. Another drawback is that certain assumptions are too strong to make the performance comparable to other methods.
\end{itemize}




%--------------------------------------------------------------------
%	SECTION 4
%--------------------------------------------------------------------
\section{Compact Network Design}
While other methods try to optimize execution and memory consumption for a given model without changing its architecture, compact network design aims at designing better models in the first place. 
\\
It is based on some empirical principles that come out over the years, through different breakthroughs in the field. A summary of the former is listed here, while an comprehensive explanation is provided in Chapter 3, where CNN architecture is discussed. 
\\
\\
The elements of an optimal CNN design are:
\begin{itemize}
    \item \emph{micro-architecture design of the network building blocks}
    
    \item \emph{$[1 \times 1]$ convolutions}
    
    \item \emph{network branching}
    
    \item \emph{depthwise separable convolution}
    
\end{itemize}



\paragraph{Considerations}
\begin{itemize}
    \item \textbf{Application}: training from scratch, most of the time. However, these principles can be combined with low-rank approximations methods when substituing a layer with a new approximated block, that could follow these rules. 
    
    \item \textbf{Drawbacks}: Since this method only aims at building better models, it does not make much sense to compare it to the others in terms of advantages and pifalls. It is just a general framework of good practices to be kept in mind
\end{itemize}


%--------------------------------------------------------------------
%	SECTION 5
%--------------------------------------------------------------------
\section{Low-rank Factorization}



\paragraph{Considerations}
\begin{itemize}
    \item \textbf{Application}: 
    
    \item \textbf{Drawbacks}: . 
\end{itemize}

%--------------------------------------------------------------------
%	SECTION 6
%--------------------------------------------------------------------
\section{Other methods}
There are several other methods to address this new trend like \emph{transferred convolutional filters}, \emph{dynamic capacity networks}, etc. For an exhaustive reference lists, please refer to \parencite{survey2017}. 

Remarkable trends are also showing up on the hardware side of the challenge, with FPGA/ASIC-based accelerators re-gaining popularity over GPUs, aiming at real-time applications, low energy consumption and high-throughput optimization 

This is important for the scope of this thesis, as the final goal of this project is to deploy optimized models on FPGA, Intel Movidius or Raspberry Pi. 

%--------------------------------------------------------------------
%	SECTION 7
%--------------------------------------------------------------------
\section{Discussion}
The presented methods are orthogonal. It is possible to combine two or three of them together to maximize the compression by applying the best suited technique on a specific scenario. For example, in the case of object detection where both convolution and fully connected layers are needed, it is feasible to compress the former with low-rank factorization and the latter with pruning. 
Moreover, when a small accuracy drop is tolerable, quantization can also be applied as a final step after the aforementioned methods. 

One pitfall, however, is that most of these methods require non trivial hyper-parameters configurations. Hence, it is not straightforward to combine them as one could get stuck in the hyper-parameters tuning loop before finding the best setup. This is probably due to the early age of these techniques. 

As for this work, since the future goal will be to optimize custom pre-trained models of the VisionLab of the University of Bologna, only two methods are feasible candidates: network pruning and low-rank factorization. Among these, the latter is the only one that provides a possible end-to-end pipeline to the problem. 
\newline 

Therefore, this thesis will focus on the low-rank factorization technique; specifically, on further developing \emph{tensor decomposition methods}, which do not seem to be fully explored yet and can have promising applications. 