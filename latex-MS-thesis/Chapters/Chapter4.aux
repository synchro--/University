\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}4 Tensor Decomposition}{1}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{1}{1}{4 Tensor Decomposition}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}{section.12}}
\newlabel{sec:bg}{{1.1}{1}{Background}{section.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A third order tensor.\relax }}{1}{figure.caption.13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tensor}{{1.1}{1}{A third order tensor.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Tensor rank}{2}{subsection.14}}
\newlabel{subsec:tensor-rank}{{1.1.1}{2}{Tensor rank}{subsection.14}{}}
\newlabel{eq:rank1-tens}{{1.1}{2}{Tensor rank}{equation.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Tensor fibers \& slices}{2}{subsection.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Fibers and slices of a tensor according to each mode.\relax }}{2}{figure.caption.17}}
\newlabel{fig:tensor-fibers}{{1.2}{2}{Fibers and slices of a tensor according to each mode.\relax }{figure.caption.17}{}}
\newlabel{eq:conv-tensor}{{1.2}{3}{Tensor fibers \& slices}{equation.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Singular value decomposition}{3}{subsection.43}}
\newlabel{eq:svd}{{1.3}{3}{Singular value decomposition}{equation.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}SVD Applications}{4}{subsection.45}}
\@writefile{toc}{\contentsline {subsubsection}{SVD on CNN}{5}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{SVD on fully-connected layers}{5}{section*.50}}
\newlabel{subsec:svd-fc}{{1.1.4}{5}{SVD on fully-connected layers}{section*.50}{}}
\newlabel{eq:fc}{{1.7}{5}{SVD on fully-connected layers}{equation.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Tensor mathematical tools}{6}{section.57}}
\newlabel{sec:tensor-math}{{1.2}{6}{Tensor mathematical tools}{section.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Basic operations}{6}{subsection.58}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Tensor unfolding or matricization along different modes.\relax }}{7}{figure.caption.61}}
\newlabel{fig:unfolding}{{1.2.1}{7}{Basic operations}{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of a \emph  {k}-mode product i.e., a tensor-matrix multiplication of a 3-dimensional tensor.\relax }}{8}{figure.caption.65}}
\newlabel{fig:k-mode}{{1.2.1}{8}{Basic operations}{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces An example of a tensor-vector product.\relax }}{8}{figure.caption.68}}
\newlabel{fig:tensor-to-vec}{{1.2.1}{8}{Basic operations}{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Representation of third-order tensor with an outer product of vectors.\relax }}{9}{figure.caption.71}}
\newlabel{fig:tensor-outer-product}{{1.6}{9}{Representation of third-order tensor with an outer product of vectors.\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Tucker Decomposition}{11}{subsection.73}}
\newlabel{eq:tucker-general}{{1.17}{11}{Tucker Decomposition}{equation.74}{}}
\newlabel{eq:tucker-3}{{1.18}{11}{Tucker Decomposition}{equation.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces A Tucker decomposition of a three-modes tensor\relax }}{11}{figure.caption.76}}
\newlabel{fig:tucker-dec}{{1.7}{11}{A Tucker decomposition of a three-modes tensor\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {subsubsection}{HO-SVD}{12}{section*.81}}
\newlabel{subsec:hosvd}{{1.2.2}{12}{HO-SVD}{section*.81}{}}
\@writefile{toc}{\contentsline {subsubsection}{Higher order orthogonal iteration}{12}{section*.85}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces An Higher Order SVD of a third-order rank-(R1-R2-R3) tensor and the different spaces, from Tensorlab \parencite {WTensorlab}.\relax }}{13}{figure.caption.84}}
\newlabel{fig:hosvd}{{1.8}{13}{An Higher Order SVD of a third-order rank-(R1-R2-R3) tensor and the different spaces, from Tensorlab \parencite {WTensorlab}.\relax }{figure.caption.84}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Canonical Polyadic Decomposition}{13}{subsection.87}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Application of tensor decompositon on CNN}{14}{section.89}}
\newlabel{sec:cpd-application}{{1.3}{14}{Application of tensor decompositon on CNN}{section.89}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Convolutional layer as 4-mode tensors}{14}{subsection.90}}
\newlabel{eq:convolution}{{1.25}{14}{Convolutional layer as 4-mode tensors}{equation.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}CP}{14}{subsection.92}}
\newlabel{eq:cpd-classic}{{1.26}{14}{CP}{equation.93}{}}
\newlabel{eq:cpd-all}{{1.27}{14}{CP}{equation.94}{}}
\newlabel{eq:cpd1}{{1.28}{15}{CP}{equation.95}{}}
\newlabel{eq:cpd2}{{1.31}{15}{CP}{equation.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Tensor Decompositions for speeding up a generalized convolution. Each box correspond to a feature map stack within a CNN, (frontal sides are spatial dimensions). Arrows show linear mappings and demonstrate how scalar values on the right are computed. Initial full convolution (A) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d \IeC {\texttimes } d window over all input maps. Jaderberg et al. (B) approximate the initial convolution as a composition of two linear mappings in which the intermediate mpa stack has R maps, being R the rank of the decomposition. Each of the two-components computes each target value with a convolution based on a spatial window of size dx1 or 1xd in all input maps. Finally, CP-decomposition (C) by Lebedev et al. approximates the convolution as a composition of four smaller convolutions: the first and the last components compute a standard 1x1 convolution that spans all input maps while the middle ones compute a 1D grouped convolution \textbf  {only on one} input map. Each box is mathematically described in equation (\ref  {eq:cpd1}-\ref  {eq:cpd2})\relax }}{15}{figure.caption.99}}
\newlabel{fig:cpd-pass}{{1.9}{15}{Tensor Decompositions for speeding up a generalized convolution. Each box correspond to a feature map stack within a CNN, (frontal sides are spatial dimensions). Arrows show linear mappings and demonstrate how scalar values on the right are computed. Initial full convolution (A) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d Ã— d window over all input maps. Jaderberg et al. (B) approximate the initial convolution as a composition of two linear mappings in which the intermediate mpa stack has R maps, being R the rank of the decomposition. Each of the two-components computes each target value with a convolution based on a spatial window of size dx1 or 1xd in all input maps. Finally, CP-decomposition (C) by Lebedev et al. approximates the convolution as a composition of four smaller convolutions: the first and the last components compute a standard 1x1 convolution that spans all input maps while the middle ones compute a 1D grouped convolution \textbf {only on one} input map. Each box is mathematically described in equation (\ref {eq:cpd1}-\ref {eq:cpd2})\relax }{figure.caption.99}{}}
\@writefile{toc}{\contentsline {subsubsection}{CPD-3}{16}{section*.100}}
\newlabel{eq:cpd1}{{1.32}{16}{CPD-3}{equation.101}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{17}{section*.104}}
\newlabel{subsec:cp-summary}{{1.3.2}{17}{Summary}{section*.104}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Tucker}{17}{subsection.113}}
\newlabel{eq:tucker-def}{{1.36}{17}{Tucker}{equation.115}{}}
\newlabel{eq:tucker1}{{1.38}{18}{Tucker}{equation.117}{}}
\newlabel{eq:tucker3}{{1.40}{18}{Tucker}{equation.119}{}}
\@writefile{toc}{\contentsline {subsubsection}{Full Tucker}{18}{section*.126}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Tucker-2 Decompositions for speeding-up a generalized convolution. Each box corresponds to a 3-way tensor $X, Z, Z^' and Y$ in equation (\ref  {eq:tucker1}-\ref  {eq:tucker3}). Arrows represent linear mappings and illustrate how each scalar value on the right is computed. Red tube, green cube and blue tube correspond to 1x1, dxd and 1x1 convolution respectively.\relax }}{19}{figure.caption.125}}
\newlabel{fig:tucker-pass}{{1.10}{19}{Tucker-2 Decompositions for speeding-up a generalized convolution. Each box corresponds to a 3-way tensor $X, Z, Z^' and Y$ in equation (\ref {eq:tucker1}-\ref {eq:tucker3}). Arrows represent linear mappings and illustrate how each scalar value on the right is computed. Red tube, green cube and blue tube correspond to 1x1, dxd and 1x1 convolution respectively.\relax }{figure.caption.125}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Complexity analysis}{20}{subsection.132}}
\newlabel{subsec:complexity}{{1.3.4}{20}{Complexity analysis}{subsection.132}{}}
\@writefile{toc}{\contentsline {subsubsection}{CP complexity}{20}{section*.133}}
\@writefile{toc}{\contentsline {subsubsection}{CP-3}{20}{section*.137}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Summary of the parameters required by the different decomposition methods analyzed.\relax }}{21}{table.caption.141}}
\newlabel{tab:compression}{{1.1}{21}{Summary of the parameters required by the different decomposition methods analyzed.\relax }{table.caption.141}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tucker}{21}{section*.139}}
\@writefile{toc}{\contentsline {paragraph}{Complexity recap}{21}{section*.140}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}In-depth discussion}{22}{section.142}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Rank estimation}{22}{subsection.143}}
\@writefile{toc}{\contentsline {subsubsection}{iterative methods}{22}{section*.144}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Evaluation of the {'rankest'} method for different sizes of input and output maps. Note that the maps sizes can be even higher in CNNs. Clearly, an iterative rank estimation approach does not scale well.\relax }}{22}{figure.caption.148}}
\newlabel{fig:rankest}{{1.11}{22}{Evaluation of the \texit {'rankest'} method for different sizes of input and output maps. Note that the maps sizes can be even higher in CNNs. Clearly, an iterative rank estimation approach does not scale well.\relax }{figure.caption.148}{}}
\@writefile{toc}{\contentsline {paragraph}{iterative compression and fine-tuning}{23}{section*.149}}
\@writefile{toc}{\contentsline {subsubsection}{Global Analytics VBMF}{23}{section*.151}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Mode-1 (top left), mode-2 (top-right), and mode-3 (bottom left) tensor unfolding of a third-order tensor. \textbf  {Not shown}: after unfolding, the rank $R$ of the resulting matrices is computed with VBMF. (Bottom right): Then, given the rank $R$, the Tucker decomposition produces a core tensor $\mathcal  {S}$ and factor matrices $A^(1)$, $A^(2)$ and $A^(3)$ of size $I_1 \times R$ $I_2 \times R$ and $I_3 \times R$ respectively. \relax }}{24}{figure.caption.152}}
\newlabel{fig:vbmf}{{1.12}{24}{Mode-1 (top left), mode-2 (top-right), and mode-3 (bottom left) tensor unfolding of a third-order tensor. \textbf {Not shown}: after unfolding, the rank $R$ of the resulting matrices is computed with VBMF. (Bottom right): Then, given the rank $R$, the Tucker decomposition produces a core tensor $\mathcal {S}$ and factor matrices $A^(1)$, $A^(2)$ and $A^(3)$ of size $I_1 \times R$ $I_2 \times R$ and $I_3 \times R$ respectively. \relax }{figure.caption.152}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variational autoencoders}{24}{section*.153}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Comparison of different optimization algorithms to compute tensor decomposition on real datasets. The method proposed by Liu et al. outperforms other methods by a large margin.\relax }}{25}{figure.caption.154}}
\newlabel{fig:VAE}{{1.13}{25}{Comparison of different optimization algorithms to compute tensor decomposition on real datasets. The method proposed by Liu et al. outperforms other methods by a large margin.\relax }{figure.caption.154}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Decomposition algorithms overview}{25}{subsection.155}}
\@writefile{toc}{\contentsline {subsubsection}{CP}{25}{section*.156}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Comparison of different optimization algorithms to compute CPD; between yellow and blue line only the init method is diverse. It is evident how ALS is indeed faster but more unstable and less precise.\relax }}{26}{figure.caption.157}}
\newlabel{fig:cp-als-nls}{{1.14}{26}{Comparison of different optimization algorithms to compute CPD; between yellow and blue line only the init method is diverse. It is evident how ALS is indeed faster but more unstable and less precise.\relax }{figure.caption.157}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Comparison of CP-NLS decomposition for different ranks: the higher the \emph  {R}-terms, the higher is the accuracy. For $R$=500 the decomposition is much more accurate than other ranks.\relax }}{27}{figure.caption.158}}
\newlabel{fig:rank-loop}{{1.15}{27}{Comparison of CP-NLS decomposition for different ranks: the higher the \emph {R}-terms, the higher is the accuracy. For $R$=500 the decomposition is much more accurate than other ranks.\relax }{figure.caption.158}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Accuracy and timing comparison between various CP-decomposition algorithms. ALS is always fast but much less accurate than CP-OPT. Image from \parencite {WCPD-talk}.\relax }}{27}{figure.caption.159}}
\newlabel{fig:cp-comparison}{{1.16}{27}{Accuracy and timing comparison between various CP-decomposition algorithms. ALS is always fast but much less accurate than CP-OPT. Image from \parencite {WCPD-talk}.\relax }{figure.caption.159}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tucker}{27}{section*.160}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}A micro-architectural view}{28}{subsection.161}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces A generic Tensor Decomposition (TP) block micro-architecture. The first and last layers performs channels squeezing/restoration while the actual convolution is performed inside these two layers. This convolution can be either separable or standard; either way the cross-channel parameters redundancy is exploited. \relax }}{29}{figure.caption.162}}
\newlabel{fig:TP-block}{{1.17}{29}{A generic Tensor Decomposition (TP) block micro-architecture. The first and last layers performs channels squeezing/restoration while the actual convolution is performed inside these two layers. This convolution can be either separable or standard; either way the cross-channel parameters redundancy is exploited. \relax }{figure.caption.162}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}TP-block model}{29}{subsection.163}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}A framework for decomposition}{29}{subsection.164}}
\newlabel{subsec:framework}{{1.4.5}{29}{A framework for decomposition}{subsection.164}{}}
\@setckpt{Chapters/Chapter4}{
\setcounter{page}{31}
\setcounter{equation}{50}
\setcounter{enumi}{8}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{1}
\setcounter{LT@tables}{2}
\setcounter{LT@chunks}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{parentequation}{0}
\setcounter{nlinenum}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{lstnumber}{24}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{43}
\setcounter{maxnames}{3}
\setcounter{minnames}{3}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{26}
\setcounter{Item}{30}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{38}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
