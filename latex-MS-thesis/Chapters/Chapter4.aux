\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}4 Tensor Decomposition}{33}{chapter.313}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{33}{4 Tensor Decomposition}{chapter.313}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Background}{33}{section.314}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Tensor mathematical tools}{33}{section.315}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Basic operations}{33}{subsection.316}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Kruskal of a tensor}{35}{subsection.325}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}CP-decompositon}{35}{subsection.326}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}HO-SVD}{35}{subsection.327}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Application of tensor decompositon on CNN}{35}{section.328}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Convolutional layer as 4-mode tensors}{35}{subsection.329}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}CPD}{35}{subsection.330}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Tucker}{35}{subsection.334}}
\newlabel{eq:tucker-def}{{4.10}{35}{Tucker}{equation.335}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}MNIST: addestramento}{36}{section.339}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}CIFAR: preprocessing}{36}{section.340}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}CIFAR: addestramento}{36}{section.341}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of k-mode multiplication on 3-dimensional tensor.\relax }}{37}{figure.caption.370}}
\newlabel{fig:k-mode}{{4.1}{37}{Example of k-mode multiplication on 3-dimensional tensor.\relax }{figure.caption.370}{}}
\newlabel{fig:training}{{4.2a}{37}{Acc.= \textasciitilde 65\% f. di attivazione = TanH\relax }{figure.caption.371}{}}
\newlabel{sub@fig:training}{{a}{37}{Acc.= \textasciitilde 65\% f. di attivazione = TanH\relax }{figure.caption.371}{}}
\newlabel{fig:validation}{{4.2b}{37}{Acc.= \textasciitilde 73\% f. di attivazione = ReLU\relax }{figure.caption.371}{}}
\newlabel{sub@fig:validation}{{b}{37}{Acc.= \textasciitilde 73\% f. di attivazione = ReLU\relax }{figure.caption.371}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Percentuali di accuracy a seconda della funzione d'attivazione. La ReLU produce indubbiamente risultati migliori.\relax }}{37}{figure.caption.371}}
\newlabel{fig:relu}{{4.2}{37}{Percentuali di accuracy a seconda della funzione d'attivazione. La ReLU produce indubbiamente risultati migliori.\relax }{figure.caption.371}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Fibers and slices of a tensor: fibers is an equivalent term for a tensor mode.\relax }}{37}{figure.caption.372}}
\newlabel{fig:tensor-fibers}{{4.3}{37}{Fibers and slices of a tensor: fibers is an equivalent term for a tensor mode.\relax }{figure.caption.372}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Tucker-2 decompositions for speeding-up a generalized convolution. Each box corresponds to a 3-way tensor $X, Z, Z^' and Y$ in equation (\ref  {eq:tucker1}-\ref  {eq:tucker3}). Arrows represent linear mappings and illustrate each scalar value on the right is computed. Red tube, green cube and blue tube correspond to 1x1, dxd and 1x1 convolution respectively.\relax }}{38}{figure.caption.373}}
\newlabel{fig:tucker-pass}{{4.4}{38}{Tucker-2 decompositions for speeding-up a generalized convolution. Each box corresponds to a 3-way tensor $X, Z, Z^' and Y$ in equation (\ref {eq:tucker1}-\ref {eq:tucker3}). Arrows represent linear mappings and illustrate each scalar value on the right is computed. Red tube, green cube and blue tube correspond to 1x1, dxd and 1x1 convolution respectively.\relax }{figure.caption.373}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Tensor decompositions for speeding up a generalized convolution. Each box correspond to a feature map stack within a CNN, (frontal sides are spatial dimensions). Arrows show linear mappings and demonstrate how scalar values on the right are computed. Initial full convolution (A) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d \IeC {\texttimes } d window over all input maps. Jaderberg et al. (B) approximate the initial convolution as a composition of two linear mappings in which the intermediate mpa stack has R maps, being R the rank of the decomposition. Each of the two-components computes each target value with a convolution based on a spatial window of size dx1 or 1xd in all input maps. Finally, CP-decomposition (C) by Lebedev et al. approximates the convolution as a composition of four smaller convolutions: the first and the last components compute a standard 1x1 convolution that spans all input maps while the middle ones compute a 1D grouped convolution \textbf  {only on one} input map.\relax }}{38}{figure.caption.374}}
\newlabel{fig:cpd-pass}{{4.5}{38}{Tensor decompositions for speeding up a generalized convolution. Each box correspond to a feature map stack within a CNN, (frontal sides are spatial dimensions). Arrows show linear mappings and demonstrate how scalar values on the right are computed. Initial full convolution (A) computes each element of the target tensor as a linear combination of the elements of a 3D subtensor that spans a spatial d Ã— d window over all input maps. Jaderberg et al. (B) approximate the initial convolution as a composition of two linear mappings in which the intermediate mpa stack has R maps, being R the rank of the decomposition. Each of the two-components computes each target value with a convolution based on a spatial window of size dx1 or 1xd in all input maps. Finally, CP-decomposition (C) by Lebedev et al. approximates the convolution as a composition of four smaller convolutions: the first and the last components compute a standard 1x1 convolution that spans all input maps while the middle ones compute a 1D grouped convolution \textbf {only on one} input map.\relax }{figure.caption.374}{}}
\@setckpt{Chapters/Chapter4}{
\setcounter{page}{39}
\setcounter{equation}{13}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{parentequation}{0}
\setcounter{nlinenum}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{lstnumber}{28}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{32}
\setcounter{maxnames}{3}
\setcounter{minnames}{3}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{31}
\setcounter{Item}{10}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{43}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
