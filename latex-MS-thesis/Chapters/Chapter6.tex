% Chapter Template

\chapter{Conclusions and future work} % Main c
\label{Chapter6}
This thesis have conducted an in-depth investigation on tensor decomposition techniques, providing insights on their application intrinsics, making them less cumbersome to use with respect to Convolutional Neural Networks. In this regard, diverse strategies have been explored, both in the number of decomposition stages and techniques and in the pipeline employed to actually perform them. Following the principles of compact and efficient design of Convolutional Neural Networks, a uniform design for Tensor Decompositions (TD) blocks has been proposed at the end of Chapter 4.
\newline 


The experimental results presented in Chapter 5 have successfully validated the proposed TD-block design, showing promising developments. Five different models have been analyzed, throughout a structured discussion about the advantages and pitfalls of one strategy respect to the other. Tucker decomposition have proven to be more stable and almost always to converge faster while providing a decent compression rate.  \\
Although CP-Decomposition needs, in general, more iterations to converge, it boasts a more effective way to achieve higher compression ratio, providing that the decomposition is accurate. If that isn't the case, it could get stuck in a local minimum and not being recovarable. To solve this problem, the experiments have shown that inserting Batch Normalization layers in-between the convolutional layers of the TD-block can help to avoid local minimum and eventually find the global one. Through these techniques, the compression ratio achieved was surprsingly high, with peaks of $(40-50 \times)$ less parameters of the baseline models. 
\newline 


As for the TD-block design only, its application as a building block of LeNet-1 in the first experiment and of CCNN in the last one have shown remarkable results. The former resulted in an overall accuracy improvement of +8 points while being $45\times$ smaller; the latter have improved the accuracy of confidence measure prediction over baseline model on the KITTI Stereo Evaluation dataset, having a fifth of the baseline parameters. \\
This last result is important in two ways: first, it shows a new way of designing CNNs that are both smaller and more effective; second, it is one of the very few results tested on a visual task different than pure image classification. 
\newline 


Many points came out that need to be addressed in future work. First of all, the rank selection. Even though a framework for tensor decomposition has been proposed in Chapter 4, the methods still requires too much manual tuning. VBMF proved to be a good tool for rank estimation when we want to preserve the accuracy; however, if a stronger compression rate is desired, manual tuning is required. \\
Moreover, the decomposition does not take into account the global optimization of all layers, which can cause  problems. 

A future direction could involve a \emph{learning-to-learn} strategy \parencite{learn} in which the optimal rank compression rate is learned by another model. A possible candidate could be reinforcement-learning. 
\newline 
Another possible research would revolve on improving the TD-block design, by adding, for example, residual learning. 

\newline 

In the lights of what has been said, tensor decomposition remains amongst the best candidate to compress CNNs in being perfect to embded into the CNN design space. 

